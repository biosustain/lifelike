version: '3.8'

services:
    pgdatabase:
        container_name: pg-database
        image: postgres:11
        command: postgres -c max_wal_size=2GB -c log_statement='all'
        environment:
            - POSTGRES_PASSWORD=postgres
        ports:
            - "5431:5432"
        networks:
            - backend

    database:
        container_name: n4j-database
        build: ./neo4j
        environment:
            - NEO4J_AUTH=neo4j/password
        volumes:
            - ./neo4j/data:/data
            - ./neo4j/logs:/logs
            - ./neo4j/import:/import
            - ./neo4j/plugins:/plugins
            - ./neo4j/backups:/backups
        ports:
            - "7474:7474"
            - "7687:7687"
        networks:
            - backend

    appserver:
        container_name: appserver
        build: ./appserver
        ports:
            - "5000:5000"
        environment:
            # Log Services
            - FORMAT_AS_JSON=false
            # Flask
            - FLASK_APP=app
            - FLASK_APP_CONFIG=Development
            - FLASK_DEBUG=1
            - FLASK_ENV=development
            # Postgres
            - POSTGRES_HOST=pgdatabase
            - POSTGRES_PORT=5432
            - POSTGRES_USER=postgres
            - POSTGRES_PASSWORD=postgres
            - POSTGRES_DB=postgres
            # ElasticSearch
            - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
            - ELASTIC_FILE_INDEX_ID=file_dev
            # Neo4j
            - NEO4J_HOST=n4j-database
            - NEO4J_AUTH=neo4j/password
            # Redis
            - REDIS_HOST=redis
            - REDIS_PORT=6379
            # Other
            - DOMAIN=http://localhost
            # Fetch this using "gsutil cp gs://kg-secrets/ansible_service_account.json ./appserver"
            - GOOGLE_APPLICATION_CREDENTIALS=ansible_service_account.json
            - LMDB_HOME_FOLDER=/home/n4j/neo4japp/services/annotations
        # Allow debugging w/ IPython or pdb
        stdin_open: true
        tty: true
        # End debugging section
        volumes:
            - ./appserver:/home/n4j
        networks:
            - frontend
            - backend
        depends_on:
            - elasticsearch
            - database
            - pgdatabase
            - redis
            - pdfparser

    nlpapi:
        container_name: nlpapi
        build:
            context: ./nlp
            dockerfile: Dockerfile
        environment:
            - FLASK_DEBUG=1
            - FLASK_APP_CONFIG=Development
            - FLASK_ENV=development
        # Allow debugging w/ IPython or pdb
        stdin_open: true
        tty: true
        # End debugging section
        ports:
            - "5001:5001"
        volumes:
            - ./nlp:/home/nlp-api-user
            - ./nlp/models:/models
        networks:
            - backend

    client:
        container_name: client
        build:
            context: ./client
            dockerfile: Dockerfile
            target: build
        command: yarn dev-start
        volumes:
            - ./client:/home/node/client
            # https://jdlm.info/articles/2019/09/06/lessons-building-node-app-docker.html
            # Helps not overwrite the node_modules with host
            - /home/node/client/node_modules
        ports:
            - "4200:4200"
        networks:
            - frontend

    redis:
        container_name: redis
        networks:
            - backend

    pdfparser:
        container_name: pdfparser
        ports:
            - "7600:7600"
        # tty: true - this is a hack for grizzly as it expects input;
        # without it, container will exit right away due to docker not accepting input by default
        tty: true
        networks:
            - backend

    cache-invalidator:
        container_name: cache-invalidator
        build: ./cache-invalidator
        volumes:
            - ./cache-invalidator:/app
        environment:
            - NEO4J_HOST=n4j-database
            - NEO4J_AUTH=neo4j/password
            - REDIS_HOST=redis
            - REDIS_PORT=6379
        depends_on:
            - database
            - redis
        networks:
            - backend

    elasticsearch:
        container_name: elasticsearch
        build:
            dockerfile: Dockerfile
            context: elasticsearch
        environment:
            - discovery.type=single-node
            - http.max_content_length=200mb  #allow 200mb of content to be sent for indexing
            - bootstrap.memory_lock=true
            - xpack.graph.enabled=false
            - xpack.watcher.enabled=false
            - xpack.license.self_generated.type=basic
        ulimits:
            memlock:
                soft: -1
                hard: -1
        ports:
            - "9200:9200"
            - "9300:9300"
        networks:
            - backend

    kibana:
        container_name: kibana
        image: docker.elastic.co/kibana/kibana:7.10.1
        ports:
            - "5601:5601"
        environment:
            SERVER_NAME: localhost
            ELASTICSEARCH_URL: http://elasticsearch:9200
        networks:
            - backend
        depends_on:
            - elasticsearch

    logstash:
        container_name: logstash
        build:
            context: logstash/
        volumes:
            - ./logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml
            - ./logstash/pipeline:/usr/share/logstash/pipeline
        ports:
            - "5044:5044"
            - "9600:9600"
        environment:
            LS_JAVA_OPTS: "-Xmx256m -Xms256m"
        networks:
            - backend
        depends_on:
            - elasticsearch

    filebeat:
        container_name: filebeat
        user: root
        build:
            context: filebeat/
        volumes:
            - /var/run/docker.sock:/var/run/docker.sock:ro
            # This is needed for filebeat to load container log path as specified in filebeat.yml
            - /var/lib/docker/containers:/var/lib/docker/containers:ro
        environment:
            - ELASTICSEARCH_HOST="http://elasticsearch:9200"
            - KIBANA_HOST="http://kibana:5601"
            - ELASTICSEARCH_USERNAME="elastic"
            - ELASTICSEARCH_PASSWORD="changeme"
        command: ["--strict.perms=false"]
        depends_on:
            - elasticsearch
            - logstash
        networks:
            - backend

networks:
    frontend:
    backend:
