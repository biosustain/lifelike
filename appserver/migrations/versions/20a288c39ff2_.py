"""Fix old annotated papers to have default annotation configs. Also removed '[]' annotations in favor for null.

Revision ID: 20a288c39ff2
Revises: 9ab4ceb163b3
Create Date: 2021-03-15 16:37:31.276178

"""
from alembic import context
from alembic import op
import sqlalchemy as sa
from sqlalchemy.sql import table, column, text, null
from sqlalchemy.dialects import postgresql
from sqlalchemy.orm.session import Session

from migrations.utils import window_chunk
from neo4japp.models import Files
from neo4japp.models.files import FileAnnotationsVersion

from neo4japp.services.annotations.constants import DEFAULT_ANNOTATION_CONFIGS

# revision identifiers, used by Alembic.
revision = '20a288c39ff2'
down_revision = '9ab4ceb163b3'
branch_labels = None
depends_on = None


def upgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    # ### end Alembic commands ###
    if context.get_x_argument(as_dictionary=True).get('data_migrate', None):
        data_upgrades()


def downgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    # ### end Alembic commands ###
    pass
    # NOTE: In practice perfect downgrades are difficult and in some cases
    # impossible! It is more practical to use database backups/snapshots to
    # "downgrade" the database. Changes to the database that we intend to
    # push to production should always be added to a NEW migration.
    # (i.e. "downgrade forward"!)


def data_upgrades():
    """Add optional data upgrade migrations here"""
    conn = op.get_bind()
    session = Session(conn)

    tableclause = table(
        'files',
        column('id', sa.Integer),
        column('annotations', postgresql.JSONB),
        column('custom_annotations', postgresql.JSONB),
        column('excluded_annotations', postgresql.JSONB),
        column('mime_type', sa.String),
        column('annotation_configs', sa.String))

    tableclause2 = table(
        'file_annotations_version',
        column('id', sa.Integer),
        column('custom_annotations', postgresql.JSONB),
        column('excluded_annotations', postgresql.JSONB))

    # need to drop the postgres default value first
    # before setting to null works
    conn.execute(text('alter table files alter column annotations drop default'))
    conn.execute(text('alter table files alter column custom_annotations drop default'))
    conn.execute(text('alter table files alter column excluded_annotations drop default'))

    conn.execute(text('alter table file_annotations_version alter column custom_annotations drop default'))
    conn.execute(text('alter table file_annotations_version alter column excluded_annotations drop default'))

    # remove the empty list string for files.annotations
    # columns are nullable so shouldn't have that value
    files = conn.execution_options(stream_results=True).execute(sa.select([
        tableclause.c.id,
        tableclause.c.annotations
    ]).where(tableclause.c.annotations == '[]'))

    for chunk in window_chunk(files, 25):
        collected = []
        for fid, file_annotations in chunk:
            collected.append({'id': fid, 'annotations': null()})
        try:
            session.bulk_update_mappings(Files, collected)
            session.commit()
        except Exception:
            session.rollback()
            raise

    # remove the empty list string for files.custom_annotations
    # columns are nullable so shouldn't have that value
    files = conn.execution_options(stream_results=True).execute(sa.select([
        tableclause.c.id,
        tableclause.c.custom_annotations
    ]).where(tableclause.c.custom_annotations == '[]'))

    for chunk in window_chunk(files, 25):
        collected = []
        for fid, custom_annotations in chunk:
            collected.append({'id': fid, 'custom_annotations': null()})
        try:
            session.bulk_update_mappings(Files, collected)
            session.commit()
        except Exception:
            session.rollback()
            raise

    # remove the empty list string for files.excluded_annotations
    # columns are nullable so shouldn't have that value
    files = conn.execution_options(stream_results=True).execute(sa.select([
        tableclause.c.id,
        tableclause.c.excluded_annotations
    ]).where(tableclause.c.excluded_annotations == '[]'))

    for chunk in window_chunk(files, 25):
        collected = []
        for fid, excluded_annotations in chunk:
            collected.append({'id': fid, 'excluded_annotations': null()})
        try:
            session.bulk_update_mappings(Files, collected)
            session.commit()
        except Exception:
            session.rollback()
            raise

    # remove the empty list string for file_annotations_version.custom_annotations
    # columns are nullable so shouldn't have that value
    files = conn.execution_options(stream_results=True).execute(sa.select([
        tableclause2.c.id,
        tableclause2.c.custom_annotations
    ]).where(tableclause2.c.custom_annotations == '[]'))

    for chunk in window_chunk(files, 25):
        collected = []
        for fid, custom_annotations in chunk:
            collected.append({'id': fid, 'custom_annotations': null()})
        try:
            session.bulk_update_mappings(FileAnnotationsVersion, collected)
            session.commit()
        except Exception:
            session.rollback()
            raise

    # remove the empty list string for file_annotations_version.excluded_annotations
    # columns are nullable so shouldn't have that value
    files = conn.execution_options(stream_results=True).execute(sa.select([
        tableclause2.c.id,
        tableclause2.c.excluded_annotations
    ]).where(tableclause2.c.excluded_annotations == '[]'))

    for chunk in window_chunk(files, 25):
        collected = []
        for fid, excluded_annotations in chunk:
            collected.append({'id': fid, 'excluded_annotations': null()})
        try:
            session.bulk_update_mappings(FileAnnotationsVersion, collected)
            session.commit()
        except Exception:
            session.rollback()
            raise

    # fix annotations, set to proper null value for files with no annotations
    files = conn.execution_options(stream_results=True).execute(sa.select([
        tableclause.c.id,
        tableclause.c.annotations
    ]).where(
        sa.and_(
            sa.or_(
                tableclause.c.mime_type == 'application/pdf',
                tableclause.c.mime_type == 'vnd.lifelike.document/enrichment-table'
            ),
            tableclause.c.annotations == 'null'
        )
    ))

    for chunk in window_chunk(files, 25):
        collected = []
        for fid, annotations in chunk:
            collected.append({'id': fid, 'annotations': null()})
        try:
            session.bulk_update_mappings(Files, collected)
            session.commit()
        except Exception:
            session.rollback()
            raise

    # update files that do not have annotation_configs to default
    files = conn.execution_options(stream_results=True).execute(sa.select([
        tableclause.c.id,
        tableclause.c.annotations
    ]).where(
        sa.and_(
            sa.or_(
                tableclause.c.mime_type == 'application/pdf',
                tableclause.c.mime_type == 'vnd.lifelike.document/enrichment-table'
            ),
            tableclause.c.annotation_configs.is_(None)
        )
    ))

    for chunk in window_chunk(files, 25):
        collected = []
        for fid, file_annotations in chunk:
            collected.append({'id': fid, 'annotation_configs': DEFAULT_ANNOTATION_CONFIGS})
        try:
            session.bulk_update_mappings(Files, collected)
            session.commit()
        except Exception:
            session.rollback()
            raise


def data_downgrades():
    """Add optional data downgrade migrations here"""
    pass
